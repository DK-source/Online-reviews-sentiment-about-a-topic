{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3127818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Duy\\anaconda3\\lib\\site-packages\\torch\\package\\_directory_reader.py:17: UserWarning: Failed to initialize NumPy: module compiled against API version 0xe but this version of numpy is 0xd (Triggered internally at  C:\\Users\\builder\\tkoch\\workspace\\pytorch\\pytorch_1647970138273\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  _dtype_to_storage = {data_type(0).dtype: data_type for data_type in _storages}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.optimizer import adam\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.metrics import BinaryAccuracy\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d1c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_train = pd.read_csv('./Datasets/yelp_review_polarity_csv/fixed_train.csv')\n",
    "rest_test = pd.read_csv('./Datasets/yelp_review_polarity_csv/fixed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df7211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length(x):\n",
    "    x = x.split()\n",
    "    return np.mean([len(i) for i in x])\n",
    "for df in [rest_train, rest_test]:\n",
    "    df['word count'] = df['review'].apply(lambda x: len(x.split()))\n",
    "    df['character count'] = df['review'].apply(lambda x: len(x))\n",
    "    df['average word length'] = df['review'].apply(average_word_length)\n",
    "    df['unique word count'] = df['review'].apply(lambda x: len(set(x.split())))\n",
    "    df['stopword count'] = df['review'].apply(lambda x: len([i for i in x.lower().split() if i in STOPWORDS]))\n",
    "    df['stopword ratio'] = df['stopword count'] / df['word count']\n",
    "    df['url count'] = df['review'].apply(lambda x: len([i for i in x.lower().split() if 'http' in i or 'https' in i]))\n",
    "\n",
    "meta_train = StandardScaler().fit_transform(rest_train.iloc[:, 2:])\n",
    "meta_test = StandardScaler().fit_transform(rest_test.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ba7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad1a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_train['review']=rest_train['review'].apply(lambda x : remove_URL(x))\n",
    "rest_train['review']=rest_train['review'].apply(lambda x : remove_html(x))\n",
    "rest_train['review']=rest_train['review'].apply(lambda x : remove_emoji(x))\n",
    "rest_train['review']=rest_train['review'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af69271",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "USE_META = True\n",
    "ADD_DENSE = False\n",
    "DENSE_DIM = 64\n",
    "ADD_DROPOUT = True\n",
    "DROPOUT = .2\n",
    "TRAIN_BASE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00bcbfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbb13994ead42978559132823d9d80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m bert_base \u001b[38;5;241m=\u001b[39m TFBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> 2\u001b[0m TOKENIZER \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "bert_base = TFBertModel.from_pretrained(model_name)\n",
    "TOKENIZER = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d71c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(data,maximum_len) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for i in range(len(data.review)):\n",
    "        encoded = TOKENIZER.encode_plus(data.review[i],\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=maximum_len,\n",
    "                                        pad_to_max_length=True,\n",
    "                                        return_attention_mask=True)\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids),np.array(attention_masks)\n",
    "def build_model(model_layer, learning_rate, use_meta = USE_META, add_dense = ADD_DENSE,\n",
    "               dense_dim = DENSE_DIM, add_dropout = ADD_DROPOUT, dropout = DROPOUT):  \n",
    "    # Inputs\n",
    "    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n",
    "    meta_input = tf.keras.Input(shape = (meta_train.shape[1], ))\n",
    "    \n",
    "    # BERT layer\n",
    "    transformer_layer = model_layer([input_ids,attention_masks])\n",
    "    \n",
    "    #choose only last hidden-state\n",
    "    output = transformer_layer[1]\n",
    "    \n",
    "    # Meta data\n",
    "    if use_meta:\n",
    "        output = tf.keras.layers.Concatenate()([output, meta_input])\n",
    "    \n",
    "    # Dense relu layer\n",
    "    if add_dense:\n",
    "        print(\"Training with additional dense layer...\")\n",
    "        output = tf.keras.layers.Dense(dense_dim,activation='relu')(output)\n",
    "    \n",
    "    # Dropout\n",
    "    if add_dropout:\n",
    "        print(\"Training with dropout...\")\n",
    "        output = tf.keras.layers.Dropout(dropout)(output)\n",
    "    \n",
    "    # Final node for binary classification\n",
    "    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n",
    "    \n",
    "    # Assemble and compile\n",
    "    if use_meta:\n",
    "        print(\"Training with meta-data...\")\n",
    "        model = tf.keras.models.Model(inputs = [input_ids,attention_masks, meta_input],outputs = output)\n",
    "    else:\n",
    "        print(\"Training without meta-data...\")\n",
    "        model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ae46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history): \n",
    "    fig, ax = plt.subplots(1, 2, figsize = (20, 10))\n",
    "\n",
    "    ax[0].plot(history.history['accuracy'], color = '#171820')\n",
    "    ax[0].plot(history.history['val_accuracy'], color = '#fdc029')\n",
    "\n",
    "    ax[1].plot(history.history['loss'], color='#171820')\n",
    "    ax[1].plot(history.history['val_loss'], color = '#fdc029')\n",
    "\n",
    "    ax[0].legend(['train', 'validation'], loc = 'upper left')\n",
    "    ax[1].legend(['train', 'validation'], loc = 'upper left')\n",
    "\n",
    "    fig.suptitle(\"Model Learning Curves\", fontsize=14)\n",
    "\n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3110c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_BASE:\n",
    "    #get our inputs\n",
    "    print('Encoding Tweets...')\n",
    "    train_input_ids,train_attention_masks = bert_encode(rest_train,60)\n",
    "    test_input_ids,test_attention_masks = bert_encode(rest_test,60)\n",
    "    print('Tweets encoded')\n",
    "    print('')\n",
    "\n",
    "    #debugging step\n",
    "    print('Train length:', len(train_input_ids))\n",
    "    print('Test length:', len(test_input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afdcda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_base = build_model(bert_base, learning_rate = 1e-5)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('base_model.h5',\n",
    "                                                monitor='val_loss', \n",
    "                                                save_best_only = True,\n",
    "                                                save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_BASE:\n",
    "    if USE_META:\n",
    "        history = BERT_base.fit([train_input_ids,train_attention_masks, meta_train], \n",
    "                                train.target, \n",
    "                                validation_split = .2, \n",
    "                                epochs = EPOCHS, \n",
    "                                callbacks = [checkpoint], \n",
    "                                batch_size = BATCH_SIZE)\n",
    "    \n",
    "    else:\n",
    "        history = BERT_base.fit([train_input_ids,train_attention_masks], \n",
    "                                train.target, \n",
    "                                validation_split = .2, \n",
    "                                epochs = EPOCHS, \n",
    "                                callbacks = [checkpoint], \n",
    "                                batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc171d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
